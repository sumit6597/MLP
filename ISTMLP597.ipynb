{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IST597_MLP_collab (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnkurMali/IST597_Spring_2022/blob/main/IST597_MLP_collab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71kdFp0QgF4K"
      },
      "source": [
        "# IST597:- Multi-Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2yHcl5xgPV1"
      },
      "source": [
        "## Load the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DPwxLR2gSLC"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from sklearn.metrics import accuracy_score\n",
        "np.random.seed(5052)\n",
        "tf.random.set_seed(5052)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "EReinRAMd-_a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV-3kEaggcO8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f6720a6-3f49-4a68-e32d-6c6e2590dc34"
      },
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##For MNIST digits "
      ],
      "metadata": {
        "id": "txSR2VclIZiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "x = (x/255).astype('float32')\n",
        "y = to_categorical(y)"
      ],
      "metadata": {
        "id": "yPR74axkRqKZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=42)"
      ],
      "metadata": {
        "id": "MzuU343EcYyh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZF1Xjnt1UUZ",
        "outputId": "d5ffa58e-8914-4d38-b430-a99bc60d049a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(59500, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU09SIsyckph",
        "outputId": "79d92e25-81d7-414d-b839-82c0de5af4bc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10500, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw78jw6pDqSM"
      },
      "source": [
        "#Get number of Gpu's and id's in the system or else you can also use Nvidia-smi in command prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dk_S2TMg_6_"
      },
      "source": [
        "## Generate random data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40XlFnwho7D8"
      },
      "source": [
        "size_input = 784\n",
        "size_hidden = [128,64]\n",
        "size_output = 10\n",
        "number_of_train_examples = 59500\n",
        "number_of_test_examples = 9500"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm23CzRihaW0"
      },
      "source": [
        "#X_train = np.random.randn(number_of_train_examples , size_input)\n",
        "#y_train = np.random.randn(number_of_train_examples)\n",
        "#X_test = np.random.randn(number_of_test_examples, size_input)\n",
        "#y_test = np.random.randn(number_of_test_examples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aigqKFFF5BM2"
      },
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(16)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(4)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb4hOoVbnzSJ"
      },
      "source": [
        "## Build MLP using Eager Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht9_qpYipgHw"
      },
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden,self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        "  def accuracy(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true = np.argmax(y_true, axis=-1)\n",
        "    y_pred = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "    # y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    # y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    # y_true_t = tf.math.argmax(y_true_tf, axis=-1)\n",
        "    # y_pred_t = tf.math.argmax(y_pred_tf, axis=-1)\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    return acc*0.1  \n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy(from_logits=True)(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "\n",
        "\n",
        "  def compute_correct_preds(self, y_pred, y_true):\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    correct = []\n",
        "    for i in range(y_pred_tf.shape[0]):\n",
        "        correct.append(tf.argmax(y_pred_tf[i]) == tf.argmax(y_true_tf[i]))\n",
        "    return np.mean(correct)  \n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUDFOuNk618X"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Running this model without regularisation 10 times with different seed"
      ],
      "metadata": {
        "id": "fkW_BVsfzCYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 10\n"
      ],
      "metadata": {
        "id": "Y0GeHDfe-coD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##without regularisation\n"
      ],
      "metadata": {
        "id": "HrBcUi1Nq9FE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden,self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        "  def accuracy(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true = np.argmax(y_true, axis=-1)\n",
        "    y_pred = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "    # y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    # y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    # y_true_t = tf.math.argmax(y_true_tf, axis=-1)\n",
        "    # y_pred_t = tf.math.argmax(y_pred_tf, axis=-1)\n",
        "\n",
        "    acc = 0.1*accuracy_score(y_true, y_pred)\n",
        "    return acc \n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy(from_logits=True)(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "\n",
        "\n",
        "  def compute_correct_preds(self, y_pred, y_true):\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    correct = []\n",
        "    for i in range(y_pred_tf.shape[0]):\n",
        "        correct.append(tf.argmax(y_pred_tf[i]) == tf.argmax(y_true_tf[i]))\n",
        "    return np.mean(correct)  \n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "KnXGZxwzhQyz"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  # lt = 0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(5647)).batch(64)\n",
        "  correct_preds = tf.constant(0, dtype=tf.float32)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_cpu.forward(inputs)\n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross Entropy:= {} - accuracy: {}'.format(epoch + 1, np.sum(loss_total) / x_train.shape[0], correct_preds))\n",
        "\n",
        "\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvHbEEDfyVPy",
        "outputId": "a3cf22fa-b750-4de1-df77-dd2c3973c66a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross Entropy:= 3.1395751050420166 - accuracy: 25.0737247467041\n",
            "Number of Epoch = 2 - Categorical Cross Entropy:= 1.4458384978991596 - accuracy: 43.67586898803711\n",
            "Number of Epoch = 3 - Categorical Cross Entropy:= 1.0250117515756303 - accuracy: 51.45920944213867\n",
            "Number of Epoch = 4 - Categorical Cross Entropy:= 0.8174493828781513 - accuracy: 55.89645767211914\n",
            "Number of Epoch = 5 - Categorical Cross Entropy:= 0.6897825630252101 - accuracy: 58.72714614868164\n",
            "Number of Epoch = 6 - Categorical Cross Entropy:= 0.6013463760504202 - accuracy: 60.89672088623047\n",
            "Number of Epoch = 7 - Categorical Cross Entropy:= 0.5358255646008403 - accuracy: 62.47374725341797\n",
            "Number of Epoch = 8 - Categorical Cross Entropy:= 0.484804293592437 - accuracy: 63.68537139892578\n",
            "Number of Epoch = 9 - Categorical Cross Entropy:= 0.4435546546743698 - accuracy: 64.69264221191406\n",
            "Number of Epoch = 10 - Categorical Cross Entropy:= 0.4094128807773109 - accuracy: 65.56449890136719\n",
            "\n",
            "Total time taken (in seconds): 292.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_cpu.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8csB9uujYgE",
        "outputId": "cb27ca36-6861-4d46-8f0d-2e1e54d30945"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0024\n",
            "Test Accuracy: 70.9810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with L1"
      ],
      "metadata": {
        "id": "3x53tmwaj9Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden,self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "\n",
        "    # self.dropout_layer = keras.layers.Dropout(rate=0.2)\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        "  def accuracy(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true = np.argmax(y_true, axis=-1)\n",
        "    y_pred = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "    # y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    # y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    # y_true_t = tf.math.argmax(y_true_tf, axis=-1)\n",
        "    # y_pred_t = tf.math.argmax(y_pred_tf, axis=-1)\n",
        "\n",
        "    acc = 0.1*accuracy_score(y_true, y_pred)\n",
        "    return acc \n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy(from_logits=True)(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L1= (tf.reduce_sum(self.W1)+ tf.reduce_sum(self.W2)+tf.reduce_sum(self.W3)) #lasso regression\n",
        "      current_loss = self.loss(predicted, y_train)+ 0.08*L1\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "\n",
        "\n",
        "  def compute_correct_preds(self, y_pred, y_true):\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    correct = []\n",
        "    for i in range(y_pred_tf.shape[0]):\n",
        "        correct.append(tf.argmax(y_pred_tf[i]) == tf.argmax(y_true_tf[i]))\n",
        "    return np.mean(correct)  \n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "6Lx5wlHFj8mn"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  # lt = 0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(2135)).batch(64)\n",
        "  correct_preds = tf.constant(0, dtype=tf.float32)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_cpu.forward(inputs)\n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    # correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "    correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross Entropy:= {} - accuracy: {}'.format(epoch + 1, np.sum(loss_total) / x_train.shape[0], correct_preds))\n",
        "\n",
        "\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avDkc5rj1vJ7",
        "outputId": "ce2ccdb1-79a4-4455-864b-398fb88e5552"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross Entropy:= 3.0689136029411763 - accuracy: 19.816909790039062\n",
            "Number of Epoch = 2 - Categorical Cross Entropy:= 1.344781118697479 - accuracy: 36.30855941772461\n",
            "Number of Epoch = 3 - Categorical Cross Entropy:= 0.8995199579831933 - accuracy: 43.65525817871094\n",
            "Number of Epoch = 4 - Categorical Cross Entropy:= 0.6881598608193278 - accuracy: 47.26934051513672\n",
            "Number of Epoch = 5 - Categorical Cross Entropy:= 0.5592137605042017 - accuracy: 49.44898223876953\n",
            "Number of Epoch = 6 - Categorical Cross Entropy:= 0.47171018251050423 - accuracy: 50.45511245727539\n",
            "Number of Epoch = 7 - Categorical Cross Entropy:= 0.4086355698529412 - accuracy: 50.98565673828125\n",
            "Number of Epoch = 8 - Categorical Cross Entropy:= 0.3608704700630252 - accuracy: 50.757362365722656\n",
            "Number of Epoch = 9 - Categorical Cross Entropy:= 0.32362129070378154 - accuracy: 50.34088134765625\n",
            "Number of Epoch = 10 - Categorical Cross Entropy:= 0.2942300091911765 - accuracy: 49.64289474487305\n",
            "\n",
            "Total time taken (in seconds): 313.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_cpu.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ4zDO1DoxQK",
        "outputId": "d6fbf43b-8335-48c5-88ac-241d543f1fd1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0017\n",
            "Test Accuracy: 53.0952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##L2 regularisation"
      ],
      "metadata": {
        "id": "kBYIanifutLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden,self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "\n",
        "    # self.dropout_layer = keras.layers.Dropout(rate=0.2)\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        "  def accuracy(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true = np.argmax(y_true, axis=-1)\n",
        "    y_pred = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "    # y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    # y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    # y_true_t = tf.math.argmax(y_true_tf, axis=-1)\n",
        "    # y_pred_t = tf.math.argmax(y_pred_tf, axis=-1)\n",
        "\n",
        "    acc = 0.1*accuracy_score(y_true, y_pred)\n",
        "    return acc \n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy(from_logits=True)(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"    \n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2=(tf.reduce_sum(tf.square(self.W1)) + tf.reduce_sum(tf.square(self.W2))\\\n",
        "+ tf.reduce_sum(tf.square(self.W3)))/3.0  #L2 Regularization\n",
        "      current_loss = self.loss(predicted, y_train)+ 0.05*L2\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "\n",
        "\n",
        "  def compute_correct_preds(self, y_pred, y_true):\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    correct = []\n",
        "    for i in range(y_pred_tf.shape[0]):\n",
        "        correct.append(tf.argmax(y_pred_tf[i]) == tf.argmax(y_true_tf[i]))\n",
        "    return np.mean(correct)  \n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "XUEjNr5Bpa2U"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  # lt = 0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(5053)).batch(64)\n",
        "  correct_preds = tf.constant(0, dtype=tf.float32)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_cpu.forward(inputs)\n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    # correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "    correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross Entropy:= {} - accuracy: {}'.format(epoch + 1, np.sum(loss_total) / x_train.shape[0], correct_preds))\n",
        "\n",
        "\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lyA9JMltHBq",
        "outputId": "dee4bc24-a5c1-4a92-fc2c-053592018029"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross Entropy:= 2.7979726890756305 - accuracy: 22.531112670898438\n",
            "Number of Epoch = 2 - Categorical Cross Entropy:= 1.2550563287815126 - accuracy: 39.32699966430664\n",
            "Number of Epoch = 3 - Categorical Cross Entropy:= 0.879578256302521 - accuracy: 47.339080810546875\n",
            "Number of Epoch = 4 - Categorical Cross Entropy:= 0.6950107011554622 - accuracy: 52.0139045715332\n",
            "Number of Epoch = 5 - Categorical Cross Entropy:= 0.5803020614495799 - accuracy: 55.20613098144531\n",
            "Number of Epoch = 6 - Categorical Cross Entropy:= 0.5004878545168068 - accuracy: 57.41677474975586\n",
            "Number of Epoch = 7 - Categorical Cross Entropy:= 0.4411460084033613 - accuracy: 59.231689453125\n",
            "Number of Epoch = 8 - Categorical Cross Entropy:= 0.395167574842437 - accuracy: 60.616661071777344\n",
            "Number of Epoch = 9 - Categorical Cross Entropy:= 0.35838826155462183 - accuracy: 61.74320602416992\n",
            "Number of Epoch = 10 - Categorical Cross Entropy:= 0.3280394235819328 - accuracy: 62.64802551269531\n",
            "\n",
            "Total time taken (in seconds): 175.31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_cpu.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3S4dRiciug0C",
        "outputId": "41ffdd9d-76cc-42d1-d07b-8c3eae37da49"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0021\n",
            "Test Accuracy: 60.3333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dropout and L2"
      ],
      "metadata": {
        "id": "aOLOxts22B9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden,self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "\n",
        "    self.dropout_layer = keras.layers.Dropout(rate=0.2)\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        "  def accuracy(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true = np.argmax(y_true, axis=-1)\n",
        "    y_pred = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "    # y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    # y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    # y_true_t = tf.math.argmax(y_true_tf, axis=-1)\n",
        "    # y_pred_t = tf.math.argmax(y_pred_tf, axis=-1)\n",
        "\n",
        "    acc = 0.1*accuracy_score(y_true, y_pred)\n",
        "    return acc \n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy(from_logits=True)(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"    \n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2=(tf.reduce_sum(tf.square(self.W1)) + tf.reduce_sum(tf.square(self.W2))\\\n",
        "+ tf.reduce_sum(tf.square(self.W3)))/3.0  #L2 Regularization\n",
        "      current_loss = self.loss(predicted, y_train)+ 0.05*L2\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "\n",
        "\n",
        "  def compute_correct_preds(self, y_pred, y_true):\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    correct = []\n",
        "    for i in range(y_pred_tf.shape[0]):\n",
        "        correct.append(tf.argmax(y_pred_tf[i]) == tf.argmax(y_true_tf[i]))\n",
        "    return np.mean(correct)  \n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    hhat2 = self.dropout_layer(hhat2)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "6wASi7pu0HN8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  # lt = 0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(5052)).batch(64)\n",
        "  correct_preds = tf.constant(0, dtype=tf.float32)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_cpu.forward(inputs)\n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    # correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "    correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross Entropy:= {} - accuracy: {}'.format(epoch + 1, np.sum(loss_total) / x_train.shape[0], correct_preds))\n",
        "\n",
        "\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdxymrSSvfZs",
        "outputId": "94e8e79a-f5df-4be8-fa1f-3ce8e54d869e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross Entropy:= 3.12915887605042 - accuracy: 25.061222076416016\n",
            "Number of Epoch = 2 - Categorical Cross Entropy:= 1.428186974789916 - accuracy: 43.63083267211914\n",
            "Number of Epoch = 3 - Categorical Cross Entropy:= 1.0041516544117648 - accuracy: 51.39716720581055\n",
            "Number of Epoch = 4 - Categorical Cross Entropy:= 0.7941912421218488 - accuracy: 55.806793212890625\n",
            "Number of Epoch = 5 - Categorical Cross Entropy:= 0.6645251444327731 - accuracy: 58.66518020629883\n",
            "Number of Epoch = 6 - Categorical Cross Entropy:= 0.574577205882353 - accuracy: 60.802669525146484\n",
            "Number of Epoch = 7 - Categorical Cross Entropy:= 0.5078742121848739 - accuracy: 62.39247131347656\n",
            "Number of Epoch = 8 - Categorical Cross Entropy:= 0.45575817358193277 - accuracy: 63.59729766845703\n",
            "Number of Epoch = 9 - Categorical Cross Entropy:= 0.4136430869222689 - accuracy: 64.5597915649414\n",
            "Number of Epoch = 10 - Categorical Cross Entropy:= 0.3785862329306723 - accuracy: 65.43932342529297\n",
            "\n",
            "Total time taken (in seconds): 171.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_cpu.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0Y7gmUA2mjx",
        "outputId": "7ee4b84f-d6ba-41ec-96b0-58d4321506c6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0022\n",
            "Test Accuracy: 70.8476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden,self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "\n",
        "    self.dropout_layer = keras.layers.Dropout(rate=0.2)\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        "  def accuracy(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true = np.argmax(y_true, axis=-1)\n",
        "    y_pred = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "    # y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    # y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    # y_true_t = tf.math.argmax(y_true_tf, axis=-1)\n",
        "    # y_pred_t = tf.math.argmax(y_pred_tf, axis=-1)\n",
        "\n",
        "    acc = 0.1*accuracy_score(y_true, y_pred)\n",
        "    return acc \n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy(from_logits=True)(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"    \n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2=(tf.reduce_sum(tf.square(self.W1)) + tf.reduce_sum(tf.square(self.W2))\\\n",
        "+ tf.reduce_sum(tf.square(self.W3)))/3.0  #L2 Regularization\n",
        "      # L1= (tf.reduce_sum(self.W1)+ tf.reduce_sum(self.W2)+tf.reduce_sum(self.W3))\n",
        "\n",
        "      current_loss = self.loss(predicted, y_train)+ 0.05*L2\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "\n",
        "\n",
        "  def compute_correct_preds(self, y_pred, y_true):\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    correct = []\n",
        "    for i in range(y_pred_tf.shape[0]):\n",
        "        correct.append(tf.argmax(y_pred_tf[i]) == tf.argmax(y_true_tf[i]))\n",
        "    return np.mean(correct)  \n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    hhat2 = self.dropout_layer(hhat2)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    # output=  tf.nn.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "5T5y87FV2Qdv"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS=20"
      ],
      "metadata": {
        "id": "gT6wt9lF4Oag"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  # lt = 0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(5052)).batch(64)\n",
        "  correct_preds = tf.constant(0, dtype=tf.float32)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_cpu.forward(inputs)\n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    # correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "    correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross Entropy:= {} - accuracy: {}'.format(epoch + 1, np.sum(loss_total) / x_train.shape[0], correct_preds))\n",
        "\n",
        "\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJewMuo-2Qb8",
        "outputId": "a1453d7c-da29-4d93-8f11-e9bc7c91af92"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross Entropy:= 3.300918067226891 - accuracy: 25.174848556518555\n",
            "Number of Epoch = 2 - Categorical Cross Entropy:= 1.5383253676470587 - accuracy: 43.44686508178711\n",
            "Number of Epoch = 3 - Categorical Cross Entropy:= 1.1019515493697478 - accuracy: 51.508514404296875\n",
            "Number of Epoch = 4 - Categorical Cross Entropy:= 0.8807383797268907 - accuracy: 56.09531021118164\n",
            "Number of Epoch = 5 - Categorical Cross Entropy:= 0.7430614495798319 - accuracy: 59.181095123291016\n",
            "Number of Epoch = 6 - Categorical Cross Entropy:= 0.6478314075630253 - accuracy: 61.43337631225586\n",
            "Number of Epoch = 7 - Categorical Cross Entropy:= 0.5771987263655463 - accuracy: 63.163818359375\n",
            "Number of Epoch = 8 - Categorical Cross Entropy:= 0.522320049894958 - accuracy: 64.52714538574219\n",
            "Number of Epoch = 9 - Categorical Cross Entropy:= 0.4780256368172269 - accuracy: 65.6704330444336\n",
            "Number of Epoch = 10 - Categorical Cross Entropy:= 0.4415176601890756 - accuracy: 66.63609313964844\n",
            "Number of Epoch = 11 - Categorical Cross Entropy:= 0.4104906118697479 - accuracy: 67.43143463134766\n",
            "Number of Epoch = 12 - Categorical Cross Entropy:= 0.3836458442752101 - accuracy: 68.14164733886719\n",
            "Number of Epoch = 13 - Categorical Cross Entropy:= 0.36010707720588236 - accuracy: 68.68534088134766\n",
            "Number of Epoch = 14 - Categorical Cross Entropy:= 0.3392532825630252 - accuracy: 69.18933868408203\n",
            "Number of Epoch = 15 - Categorical Cross Entropy:= 0.3206168264180672 - accuracy: 69.67052459716797\n",
            "Number of Epoch = 16 - Categorical Cross Entropy:= 0.3039157037815126 - accuracy: 70.1181869506836\n",
            "Number of Epoch = 17 - Categorical Cross Entropy:= 0.28881821165966387 - accuracy: 70.5783462524414\n",
            "Number of Epoch = 18 - Categorical Cross Entropy:= 0.27491237198004204 - accuracy: 70.90933990478516\n",
            "Number of Epoch = 19 - Categorical Cross Entropy:= 0.2621665244222689 - accuracy: 71.2369384765625\n",
            "Number of Epoch = 20 - Categorical Cross Entropy:= 0.2503812040441176 - accuracy: 71.51663970947266\n",
            "\n",
            "Total time taken (in seconds): 638.09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_cpu.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN6RWgL12QaR",
        "outputId": "852a3c16-d52d-4949-c388-c2c64746410b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0016\n",
            "Test Accuracy: 76.3048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  # lt = 0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(1234)).batch(64)\n",
        "  correct_preds = tf.constant(0, dtype=tf.float32)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_cpu.forward(inputs)\n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    # correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "    correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross Entropy:= {} - accuracy: {}'.format(epoch + 1, np.sum(loss_total) / x_train.shape[0], correct_preds))\n",
        "\n",
        "\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVLFt_tx2QRu",
        "outputId": "2d24cfc4-2857-45c1-8241-bdb9aca6af35"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross Entropy:= 3.151113182773109 - accuracy: 21.94361114501953\n",
            "Number of Epoch = 2 - Categorical Cross Entropy:= 1.4414266018907562 - accuracy: 40.0103874206543\n",
            "Number of Epoch = 3 - Categorical Cross Entropy:= 0.9975117515756302 - accuracy: 48.72641372680664\n",
            "Number of Epoch = 4 - Categorical Cross Entropy:= 0.7817564338235294 - accuracy: 53.813232421875\n",
            "Number of Epoch = 5 - Categorical Cross Entropy:= 0.6499432773109244 - accuracy: 57.06222152709961\n",
            "Number of Epoch = 6 - Categorical Cross Entropy:= 0.5598379070378151 - accuracy: 59.441349029541016\n",
            "Number of Epoch = 7 - Categorical Cross Entropy:= 0.49384791885504203 - accuracy: 61.393341064453125\n",
            "Number of Epoch = 8 - Categorical Cross Entropy:= 0.44318582589285715 - accuracy: 62.693511962890625\n",
            "Number of Epoch = 9 - Categorical Cross Entropy:= 0.40267542016806723 - accuracy: 63.830108642578125\n",
            "Number of Epoch = 10 - Categorical Cross Entropy:= 0.36933560924369746 - accuracy: 64.80890655517578\n",
            "Number of Epoch = 11 - Categorical Cross Entropy:= 0.3412357536764706 - accuracy: 65.63692474365234\n",
            "Number of Epoch = 12 - Categorical Cross Entropy:= 0.31723069852941177 - accuracy: 66.38241577148438\n",
            "Number of Epoch = 13 - Categorical Cross Entropy:= 0.2964337907037815 - accuracy: 66.8808822631836\n",
            "Number of Epoch = 14 - Categorical Cross Entropy:= 0.27817972032563026 - accuracy: 67.43480682373047\n",
            "Number of Epoch = 15 - Categorical Cross Entropy:= 0.2620175124737395 - accuracy: 67.91127014160156\n",
            "Number of Epoch = 16 - Categorical Cross Entropy:= 0.24752862394957983 - accuracy: 68.34871673583984\n",
            "Number of Epoch = 17 - Categorical Cross Entropy:= 0.23443326549369747 - accuracy: 68.7167739868164\n",
            "Number of Epoch = 18 - Categorical Cross Entropy:= 0.22249517463235294 - accuracy: 69.00878143310547\n",
            "Number of Epoch = 19 - Categorical Cross Entropy:= 0.21163254989495797 - accuracy: 69.28780364990234\n",
            "Number of Epoch = 20 - Categorical Cross Entropy:= 0.20164417016806724 - accuracy: 69.61128997802734\n",
            "\n",
            "Total time taken (in seconds): 663.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_cpu.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNSFgAqcvfXV",
        "outputId": "571851a3-f62e-45c7-d8d3-816ba08eb6ce"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0012\n",
            "Test Accuracy: 74.9238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##adam + dropout + L2 + 20epochs\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_bj5ROZCCbRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden,self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "\n",
        "    self.dropout_layer = keras.layers.Dropout(rate=0.2)\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        "  def accuracy(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true = np.argmax(y_true, axis=-1)\n",
        "    y_pred = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "    # y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    # y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    # y_true_t = tf.math.argmax(y_true_tf, axis=-1)\n",
        "    # y_pred_t = tf.math.argmax(y_pred_tf, axis=-1)\n",
        "\n",
        "    acc = 0.1*accuracy_score(y_true, y_pred)\n",
        "    return acc \n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy(from_logits=True)(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"    \n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2=(tf.reduce_sum(tf.square(self.W1)) + tf.reduce_sum(tf.square(self.W2))\\\n",
        "+ tf.reduce_sum(tf.square(self.W3)))/3.0  #L2 Regularization\n",
        "      # L1= (tf.reduce_sum(self.W1)+ tf.reduce_sum(self.W2)+tf.reduce_sum(self.W3))\n",
        "\n",
        "      current_loss = self.loss(predicted, y_train)+ 0.05*L2\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "\n",
        "\n",
        "  def compute_correct_preds(self, y_pred, y_true):\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    correct = []\n",
        "    for i in range(y_pred_tf.shape[0]):\n",
        "        correct.append(tf.argmax(y_pred_tf[i]) == tf.argmax(y_true_tf[i]))\n",
        "    return np.mean(correct)  \n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    hhat2 = self.dropout_layer(hhat2)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    # output=  tf.nn.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "LogSl7qvvfVm"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  # lt = 0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(1234)).batch(64)\n",
        "  correct_preds = tf.constant(0, dtype=tf.float32)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_cpu.forward(inputs)\n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    # correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "    correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross Entropy:= {} - accuracy: {}'.format(epoch + 1, np.sum(loss_total) / x_train.shape[0], correct_preds))\n",
        "\n",
        "\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2llyvSavfTb",
        "outputId": "a74a730d-1ec7-46cb-e132-9ee6f165cda1"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross Entropy:= 2.935300945378151 - accuracy: 27.560091018676758\n",
            "Number of Epoch = 2 - Categorical Cross Entropy:= 1.0541473214285715 - accuracy: 51.966041564941406\n",
            "Number of Epoch = 3 - Categorical Cross Entropy:= 0.6887385766806723 - accuracy: 61.45254898071289\n",
            "Number of Epoch = 4 - Categorical Cross Entropy:= 0.5221702665441177 - accuracy: 66.25068664550781\n",
            "Number of Epoch = 5 - Categorical Cross Entropy:= 0.4199053637079832 - accuracy: 69.22969055175781\n",
            "Number of Epoch = 6 - Categorical Cross Entropy:= 0.34881666885504203 - accuracy: 71.32286071777344\n",
            "Number of Epoch = 7 - Categorical Cross Entropy:= 0.2957212119222689 - accuracy: 72.88422393798828\n",
            "Number of Epoch = 8 - Categorical Cross Entropy:= 0.2538384814863445 - accuracy: 73.99596405029297\n",
            "Number of Epoch = 9 - Categorical Cross Entropy:= 0.2193883600315126 - accuracy: 74.99038696289062\n",
            "Number of Epoch = 10 - Categorical Cross Entropy:= 0.19037760963760506 - accuracy: 75.77001190185547\n",
            "Number of Epoch = 11 - Categorical Cross Entropy:= 0.16592397584033614 - accuracy: 76.43484497070312\n",
            "Number of Epoch = 12 - Categorical Cross Entropy:= 0.14495127035189076 - accuracy: 76.8967514038086\n",
            "Number of Epoch = 13 - Categorical Cross Entropy:= 0.12679337907037816 - accuracy: 77.29119873046875\n",
            "Number of Epoch = 14 - Categorical Cross Entropy:= 0.11079966517857143 - accuracy: 77.7083969116211\n",
            "Number of Epoch = 15 - Categorical Cross Entropy:= 0.09678051962972689 - accuracy: 77.99014282226562\n",
            "Number of Epoch = 16 - Categorical Cross Entropy:= 0.08454992778361345 - accuracy: 78.33645629882812\n",
            "Number of Epoch = 17 - Categorical Cross Entropy:= 0.07378536961659664 - accuracy: 78.48096466064453\n",
            "Number of Epoch = 18 - Categorical Cross Entropy:= 0.06424071034663865 - accuracy: 78.58706665039062\n",
            "Number of Epoch = 19 - Categorical Cross Entropy:= 0.05582617597820378 - accuracy: 78.75215148925781\n",
            "Number of Epoch = 20 - Categorical Cross Entropy:= 0.048396939830619747 - accuracy: 78.77240753173828\n",
            "\n",
            "Total time taken (in seconds): 920.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_cpu.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/(yPredMax.shape[0])\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VKn-vU_vfRK",
        "outputId": "203fd044-4cf3-4e89-ccd1-f3305a84368e"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0003\n",
            "Test Accuracy: 84.1143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##For MNIST fashion dataset"
      ],
      "metadata": {
        "id": "77FdoRUKIR_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = fetch_openml('Fashion-MNIST', version=1, return_X_y=True)\n",
        "x = (x/255).astype('float32')\n",
        "y = to_categorical(y)"
      ],
      "metadata": {
        "id": "BQoKJz47vfOc"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=42)"
      ],
      "metadata": {
        "id": "xFF8DZ4NvfL_"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size_input = 784\n",
        "size_hidden = [128,64]\n",
        "size_output = 10\n",
        "number_of_train_examples = 59500\n",
        "number_of_test_examples = 9500"
      ],
      "metadata": {
        "id": "aYrx5575CkMd"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(16)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(4)"
      ],
      "metadata": {
        "id": "pz1OqC5OCkKk"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##without regularisation "
      ],
      "metadata": {
        "id": "OpIPhIAjKkuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden,self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        "  def accuracy(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true = np.argmax(y_true, axis=-1)\n",
        "    y_pred = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "    # y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    # y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    # y_true_t = tf.math.argmax(y_true_tf, axis=-1)\n",
        "    # y_pred_t = tf.math.argmax(y_pred_tf, axis=-1)\n",
        "\n",
        "    acc = 0.1*accuracy_score(y_true, y_pred)\n",
        "    return acc \n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy(from_logits=True)(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "\n",
        "\n",
        "  def compute_correct_preds(self, y_pred, y_true):\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    correct = []\n",
        "    for i in range(y_pred_tf.shape[0]):\n",
        "        correct.append(tf.argmax(y_pred_tf[i]) == tf.argmax(y_true_tf[i]))\n",
        "    return np.mean(correct)  \n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "bWf-0OxKCkIQ"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS=10"
      ],
      "metadata": {
        "id": "tlSzowI4Kzip"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  # lt = 0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(5647)).batch(64)\n",
        "  correct_preds = tf.constant(0, dtype=tf.float32)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_cpu.forward(inputs)\n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross Entropy:= {} - accuracy: {}'.format(epoch + 1, np.sum(loss_total) / x_train.shape[0], correct_preds))\n",
        "\n",
        "\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2NGguPKCkGD",
        "outputId": "40ed28e6-0328-4c26-f494-de1b916b66d1"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross Entropy:= 2.594418855042017 - accuracy: 33.51433563232422\n",
            "Number of Epoch = 2 - Categorical Cross Entropy:= 1.073673713235294 - accuracy: 49.68817901611328\n",
            "Number of Epoch = 3 - Categorical Cross Entropy:= 0.7893897058823529 - accuracy: 54.84105682373047\n",
            "Number of Epoch = 4 - Categorical Cross Entropy:= 0.6423022584033613 - accuracy: 57.92373275756836\n",
            "Number of Epoch = 5 - Categorical Cross Entropy:= 0.5524031643907563 - accuracy: 59.89958572387695\n",
            "Number of Epoch = 6 - Categorical Cross Entropy:= 0.49047022715336136 - accuracy: 61.330101013183594\n",
            "Number of Epoch = 7 - Categorical Cross Entropy:= 0.44415503545168067 - accuracy: 62.269134521484375\n",
            "Number of Epoch = 8 - Categorical Cross Entropy:= 0.4073125 - accuracy: 62.96989059448242\n",
            "Number of Epoch = 9 - Categorical Cross Entropy:= 0.37726962972689077 - accuracy: 63.5167121887207\n",
            "Number of Epoch = 10 - Categorical Cross Entropy:= 0.3518142069327731 - accuracy: 63.95026779174805\n",
            "\n",
            "Total time taken (in seconds): 288.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_cpu.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTFcAewzCkDl",
        "outputId": "4ad02d3e-70e3-4581-d794-9f1c620225fe"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0022\n",
            "Test Accuracy: 68.5714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##L1 regularisation"
      ],
      "metadata": {
        "id": "tSH4CtzWOHkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden,self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "\n",
        "    # self.dropout_layer = keras.layers.Dropout(rate=0.2)\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        "  def accuracy(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true = np.argmax(y_true, axis=-1)\n",
        "    y_pred = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "    # y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    # y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    # y_true_t = tf.math.argmax(y_true_tf, axis=-1)\n",
        "    # y_pred_t = tf.math.argmax(y_pred_tf, axis=-1)\n",
        "\n",
        "    acc = 0.1*accuracy_score(y_true, y_pred)\n",
        "    return acc \n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy(from_logits=True)(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L1= (tf.reduce_sum(self.W1)+ tf.reduce_sum(self.W2)+tf.reduce_sum(self.W3)) #lasso regression\n",
        "      current_loss = self.loss(predicted, y_train)+ 0.08*L1\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "\n",
        "\n",
        "  def compute_correct_preds(self, y_pred, y_true):\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    correct = []\n",
        "    for i in range(y_pred_tf.shape[0]):\n",
        "        correct.append(tf.argmax(y_pred_tf[i]) == tf.argmax(y_true_tf[i]))\n",
        "    return np.mean(correct)  \n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "rKbE408xKO5S"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  # lt = 0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(2135)).batch(64)\n",
        "  correct_preds = tf.constant(0, dtype=tf.float32)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_cpu.forward(inputs)\n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    # correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "    correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross Entropy:= {} - accuracy: {}'.format(epoch + 1, np.sum(loss_total) / x_train.shape[0], correct_preds))\n",
        "\n",
        "\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeMVA9aLKO3M",
        "outputId": "e4f13951-014a-41d0-bd0b-cc3c9e8035c1"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross Entropy:= 2.021857405462185 - accuracy: 35.10311508178711\n",
            "Number of Epoch = 2 - Categorical Cross Entropy:= 0.8614448529411765 - accuracy: 49.55231857299805\n",
            "Number of Epoch = 3 - Categorical Cross Entropy:= 0.6090793723739496 - accuracy: 53.14997482299805\n",
            "Number of Epoch = 4 - Categorical Cross Entropy:= 0.4715764180672269 - accuracy: 54.71477508544922\n",
            "Number of Epoch = 5 - Categorical Cross Entropy:= 0.38734378282563026 - accuracy: 55.00724792480469\n",
            "Number of Epoch = 6 - Categorical Cross Entropy:= 0.3308364298844538 - accuracy: 54.675655364990234\n",
            "Number of Epoch = 7 - Categorical Cross Entropy:= 0.29185894826680675 - accuracy: 53.4860725402832\n",
            "Number of Epoch = 8 - Categorical Cross Entropy:= 0.2640085839023109 - accuracy: 52.11960983276367\n",
            "Number of Epoch = 9 - Categorical Cross Entropy:= 0.24333093159138655 - accuracy: 50.172691345214844\n",
            "Number of Epoch = 10 - Categorical Cross Entropy:= 0.22546486016281514 - accuracy: 47.98508071899414\n",
            "\n",
            "Total time taken (in seconds): 569.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_cpu.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Tm2lu_xKO07",
        "outputId": "c8993867-9ba5-4695-cda8-6426bc0edee5"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0014\n",
            "Test Accuracy: 50.0190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##L2 regularisation"
      ],
      "metadata": {
        "id": "QooL0-NSMTGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden,self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "\n",
        "    # self.dropout_layer = keras.layers.Dropout(rate=0.2)\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        "  def accuracy(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true = np.argmax(y_true, axis=-1)\n",
        "    y_pred = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "    # y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    # y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    # y_true_t = tf.math.argmax(y_true_tf, axis=-1)\n",
        "    # y_pred_t = tf.math.argmax(y_pred_tf, axis=-1)\n",
        "\n",
        "    acc = 0.1*accuracy_score(y_true, y_pred)\n",
        "    return acc \n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy(from_logits=True)(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"    \n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2=(tf.reduce_sum(tf.square(self.W1)) + tf.reduce_sum(tf.square(self.W2))\\\n",
        "+ tf.reduce_sum(tf.square(self.W3)))/3.0  #L2 Regularization\n",
        "      current_loss = self.loss(predicted, y_train)+ 0.05*L2\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "\n",
        "\n",
        "  def compute_correct_preds(self, y_pred, y_true):\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    correct = []\n",
        "    for i in range(y_pred_tf.shape[0]):\n",
        "        correct.append(tf.argmax(y_pred_tf[i]) == tf.argmax(y_true_tf[i]))\n",
        "    return np.mean(correct)  \n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "GHAZNIuvMSJ2"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  # lt = 0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(5052)).batch(64)\n",
        "  correct_preds = tf.constant(0, dtype=tf.float32)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_cpu.forward(inputs)\n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    # correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "    correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross Entropy:= {} - accuracy: {}'.format(epoch + 1, np.sum(loss_total) / x_train.shape[0], correct_preds))\n",
        "\n",
        "\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_l-8dccMSHw",
        "outputId": "4015c1d3-71aa-4017-a235-6322d3f35969"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross Entropy:= 2.9998970588235294 - accuracy: 30.856393814086914\n",
            "Number of Epoch = 2 - Categorical Cross Entropy:= 1.1434875262605042 - accuracy: 49.85059356689453\n",
            "Number of Epoch = 3 - Categorical Cross Entropy:= 0.8237160582983193 - accuracy: 55.42241287231445\n",
            "Number of Epoch = 4 - Categorical Cross Entropy:= 0.6654498424369748 - accuracy: 58.120018005371094\n",
            "Number of Epoch = 5 - Categorical Cross Entropy:= 0.5676387867647059 - accuracy: 59.8011589050293\n",
            "Number of Epoch = 6 - Categorical Cross Entropy:= 0.4990478269432773 - accuracy: 60.848167419433594\n",
            "Number of Epoch = 7 - Categorical Cross Entropy:= 0.446800912552521 - accuracy: 61.62852478027344\n",
            "Number of Epoch = 8 - Categorical Cross Entropy:= 0.4055203847163866 - accuracy: 62.172874450683594\n",
            "Number of Epoch = 9 - Categorical Cross Entropy:= 0.37186045824579833 - accuracy: 62.62528991699219\n",
            "Number of Epoch = 10 - Categorical Cross Entropy:= 0.34306535582983194 - accuracy: 62.954776763916016\n",
            "\n",
            "Total time taken (in seconds): 307.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_cpu.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqhNTziEMSGB",
        "outputId": "961373b6-cbba-46b7-fe90-45aa132e25e2"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0021\n",
            "Test Accuracy: 67.5048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dropout + L2"
      ],
      "metadata": {
        "id": "fFSASfjANBPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden,self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "\n",
        "    self.dropout_layer = keras.layers.Dropout(rate=0.2)\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        "  def accuracy(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true = np.argmax(y_true, axis=-1)\n",
        "    y_pred = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "    # y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    # y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    # y_true_t = tf.math.argmax(y_true_tf, axis=-1)\n",
        "    # y_pred_t = tf.math.argmax(y_pred_tf, axis=-1)\n",
        "\n",
        "    acc = 0.1*accuracy_score(y_true, y_pred)\n",
        "    return acc \n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy(from_logits=True)(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"    \n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2=(tf.reduce_sum(tf.square(self.W1)) + tf.reduce_sum(tf.square(self.W2))\\\n",
        "+ tf.reduce_sum(tf.square(self.W3)))/3.0  #L2 Regularization\n",
        "      current_loss = self.loss(predicted, y_train)+ 0.05*L2\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "\n",
        "\n",
        "  def compute_correct_preds(self, y_pred, y_true):\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    correct = []\n",
        "    for i in range(y_pred_tf.shape[0]):\n",
        "        correct.append(tf.argmax(y_pred_tf[i]) == tf.argmax(y_true_tf[i]))\n",
        "    return np.mean(correct)  \n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    hhat2 = self.dropout_layer(hhat2)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "Yk0xiDVIMSCg"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  # lt = 0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(5052)).batch(64)\n",
        "  correct_preds = tf.constant(0, dtype=tf.float32)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_cpu.forward(inputs)\n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    # correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "    correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross Entropy:= {} - accuracy: {}'.format(epoch + 1, np.sum(loss_total) / x_train.shape[0], correct_preds))\n",
        "\n",
        "\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LOJAEPcMR_P",
        "outputId": "21c68715-63e0-4366-bc3c-8c0e23413fc5"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross Entropy:= 2.6322481617647058 - accuracy: 34.36788558959961\n",
            "Number of Epoch = 2 - Categorical Cross Entropy:= 1.0718106617647059 - accuracy: 52.229583740234375\n",
            "Number of Epoch = 3 - Categorical Cross Entropy:= 0.8162452731092437 - accuracy: 57.16517639160156\n",
            "Number of Epoch = 4 - Categorical Cross Entropy:= 0.6772162552521008 - accuracy: 59.6036491394043\n",
            "Number of Epoch = 5 - Categorical Cross Entropy:= 0.5838434217436975 - accuracy: 61.1396598815918\n",
            "Number of Epoch = 6 - Categorical Cross Entropy:= 0.5160455291491597 - accuracy: 62.19888687133789\n",
            "Number of Epoch = 7 - Categorical Cross Entropy:= 0.46356131827731095 - accuracy: 63.00508117675781\n",
            "Number of Epoch = 8 - Categorical Cross Entropy:= 0.42146323529411767 - accuracy: 63.59400177001953\n",
            "Number of Epoch = 9 - Categorical Cross Entropy:= 0.38661177127100843 - accuracy: 64.05372619628906\n",
            "Number of Epoch = 10 - Categorical Cross Entropy:= 0.3569582786239496 - accuracy: 64.46502685546875\n",
            "\n",
            "Total time taken (in seconds): 321.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_cpu.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM16WdsNMR85",
        "outputId": "ae175a4a-a7af-44ff-d572-17a844ea60c2"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0022\n",
            "Test Accuracy: 68.8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "same as above + 20 epochs"
      ],
      "metadata": {
        "id": "1XfkstVsNWom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden,self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "\n",
        "    self.dropout_layer = keras.layers.Dropout(rate=0.2)\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        "  def accuracy(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true = np.argmax(y_true, axis=-1)\n",
        "    y_pred = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "    # y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    # y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    # y_true_t = tf.math.argmax(y_true_tf, axis=-1)\n",
        "    # y_pred_t = tf.math.argmax(y_pred_tf, axis=-1)\n",
        "\n",
        "    acc = 0.1*accuracy_score(y_true, y_pred)\n",
        "    return acc \n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy(from_logits=True)(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"    \n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2=(tf.reduce_sum(tf.square(self.W1)) + tf.reduce_sum(tf.square(self.W2))\\\n",
        "+ tf.reduce_sum(tf.square(self.W3)))/3.0  #L2 Regularization\n",
        "      # L1= (tf.reduce_sum(self.W1)+ tf.reduce_sum(self.W2)+tf.reduce_sum(self.W3))\n",
        "\n",
        "      current_loss = self.loss(predicted, y_train)+ 0.05*L2\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "\n",
        "\n",
        "  def compute_correct_preds(self, y_pred, y_true):\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    correct = []\n",
        "    for i in range(y_pred_tf.shape[0]):\n",
        "        correct.append(tf.argmax(y_pred_tf[i]) == tf.argmax(y_true_tf[i]))\n",
        "    return np.mean(correct)  \n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    hhat2 = self.dropout_layer(hhat2)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    # output=  tf.nn.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "hwGYtTJ8MR6g"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS=20"
      ],
      "metadata": {
        "id": "tihaTZPhNb-C"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  # lt = 0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(5052)).batch(64)\n",
        "  correct_preds = tf.constant(0, dtype=tf.float32)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_cpu.forward(inputs)\n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    # correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "    correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross Entropy:= {} - accuracy: {}'.format(epoch + 1, np.sum(loss_total) / x_train.shape[0], correct_preds))\n",
        "\n",
        "\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR9ocE8UMR4G",
        "outputId": "60cf74eb-2028-457a-c275-5bc90e855e1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross Entropy:= 2.2203742121848737 - accuracy: 33.75864791870117\n",
            "Number of Epoch = 2 - Categorical Cross Entropy:= 0.9518251706932773 - accuracy: 50.83108901977539\n",
            "Number of Epoch = 3 - Categorical Cross Entropy:= 0.724858849789916 - accuracy: 55.42619705200195\n",
            "Number of Epoch = 4 - Categorical Cross Entropy:= 0.6020840336134454 - accuracy: 57.799564361572266\n",
            "Number of Epoch = 5 - Categorical Cross Entropy:= 0.51965625 - accuracy: 59.39543914794922\n",
            "Number of Epoch = 6 - Categorical Cross Entropy:= 0.4593531381302521 - accuracy: 60.60563278198242\n",
            "Number of Epoch = 7 - Categorical Cross Entropy:= 0.41313409269957985 - accuracy: 61.50468063354492\n",
            "Number of Epoch = 8 - Categorical Cross Entropy:= 0.37563461790966385 - accuracy: 62.25014114379883\n",
            "Number of Epoch = 9 - Categorical Cross Entropy:= 0.34485619091386555 - accuracy: 62.79432678222656\n",
            "Number of Epoch = 10 - Categorical Cross Entropy:= 0.3191509978991597 - accuracy: 63.26117706298828\n",
            "Number of Epoch = 11 - Categorical Cross Entropy:= 0.2972275800945378 - accuracy: 63.630096435546875\n",
            "Number of Epoch = 12 - Categorical Cross Entropy:= 0.27791573660714286 - accuracy: 63.91339111328125\n",
            "Number of Epoch = 13 - Categorical Cross Entropy:= 0.26084473476890757 - accuracy: 64.20411682128906\n",
            "Number of Epoch = 14 - Categorical Cross Entropy:= 0.24548723082983193 - accuracy: 64.41500854492188\n",
            "Number of Epoch = 15 - Categorical Cross Entropy:= 0.23172585674894958 - accuracy: 64.5868911743164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##adam + dropout + L2 + 20epochs\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bhQ5MqynNwuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden,self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "\n",
        "    self.dropout_layer = keras.layers.Dropout(rate=0.2)\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        "  def accuracy(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true = np.argmax(y_true, axis=-1)\n",
        "    y_pred = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "    # y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    # y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    # y_true_t = tf.math.argmax(y_true_tf, axis=-1)\n",
        "    # y_pred_t = tf.math.argmax(y_pred_tf, axis=-1)\n",
        "\n",
        "    acc = 0.1*accuracy_score(y_true, y_pred)\n",
        "    return acc \n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy(from_logits=True)(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"    \n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2=(tf.reduce_sum(tf.square(self.W1)) + tf.reduce_sum(tf.square(self.W2))\\\n",
        "+ tf.reduce_sum(tf.square(self.W3)))/3.0  #L2 Regularization\n",
        "      # L1= (tf.reduce_sum(self.W1)+ tf.reduce_sum(self.W2)+tf.reduce_sum(self.W3))\n",
        "\n",
        "      current_loss = self.loss(predicted, y_train)+ 0.05*L2\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "\n",
        "\n",
        "  def compute_correct_preds(self, y_pred, y_true):\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    correct = []\n",
        "    for i in range(y_pred_tf.shape[0]):\n",
        "        correct.append(tf.argmax(y_pred_tf[i]) == tf.argmax(y_true_tf[i]))\n",
        "    return np.mean(correct)  \n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    hhat2 = self.dropout_layer(hhat2)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    # output=  tf.nn.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "37SnW8ikMRz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  # lt = 0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(1234)).batch(64)\n",
        "  correct_preds = tf.constant(0, dtype=tf.float32)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_cpu.forward(inputs)\n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    # correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "    correct_preds = correct_preds + mlp_on_cpu.accuracy(preds, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross Entropy:= {} - accuracy: {}'.format(epoch + 1, np.sum(loss_total) / x_train.shape[0], correct_preds))\n",
        "\n",
        "\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Sp_pForMRuu",
        "outputId": "5086e8b0-2e09-4560-d9e9-5f7289dc7c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross Entropy:= 3.0630869222689077 - accuracy: 31.22429847717285\n",
            "Number of Epoch = 2 - Categorical Cross Entropy:= 0.9326937368697479 - accuracy: 51.36847686767578\n",
            "Number of Epoch = 3 - Categorical Cross Entropy:= 0.6465136554621849 - accuracy: 57.98876953125\n",
            "Number of Epoch = 4 - Categorical Cross Entropy:= 0.5049840467436975 - accuracy: 61.35936737060547\n",
            "Number of Epoch = 5 - Categorical Cross Entropy:= 0.4176030396533613 - accuracy: 63.63134765625\n",
            "Number of Epoch = 6 - Categorical Cross Entropy:= 0.35523880646008404 - accuracy: 65.23921966552734\n",
            "Number of Epoch = 7 - Categorical Cross Entropy:= 0.30724960609243696 - accuracy: 66.36721801757812\n",
            "Number of Epoch = 8 - Categorical Cross Entropy:= 0.2695611869747899 - accuracy: 67.1525650024414\n",
            "Number of Epoch = 9 - Categorical Cross Entropy:= 0.2391012506565126 - accuracy: 67.80240631103516\n",
            "Number of Epoch = 10 - Categorical Cross Entropy:= 0.21353820903361345 - accuracy: 68.2868423461914\n",
            "Number of Epoch = 11 - Categorical Cross Entropy:= 0.1916102612920168 - accuracy: 68.73299407958984\n",
            "Number of Epoch = 12 - Categorical Cross Entropy:= 0.17281727612920167 - accuracy: 69.06803894042969\n",
            "Number of Epoch = 13 - Categorical Cross Entropy:= 0.15632382484243698 - accuracy: 69.40330505371094\n",
            "Number of Epoch = 14 - Categorical Cross Entropy:= 0.14198573726365546 - accuracy: 69.63285064697266\n",
            "Number of Epoch = 15 - Categorical Cross Entropy:= 0.12868769695378152 - accuracy: 69.90099334716797\n",
            "Number of Epoch = 16 - Categorical Cross Entropy:= 0.1169072265625 - accuracy: 70.08745574951172\n",
            "Number of Epoch = 17 - Categorical Cross Entropy:= 0.10640522419905463 - accuracy: 70.28749084472656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_cpu.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fiCb0hZMRsL",
        "outputId": "27014840-ed1e-4103-84a3-e7ae1760e841"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0022\n",
            "Test Accuracy: 70.8476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "accuracy_base = np.array([56.80, 47.44, 47.81, 47.41, 51.20, 44.70, 46.86, 45.25, 45.44, 45.56])/100.0\n",
        "mean_base = np.sum(accuracy_base)/accuracy_base.shape[0]\n",
        "standard_dev_base = np.sqrt(np.sum((accuracy_base-mean_base)**2)/(accuracy_base.shape[0]-1.0))\n",
        "standard_error_base = standard_dev_base/np.sqrt(accuracy_base.shape[0])\n",
        "variance_base = standard_dev_base**2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "accuracy_L1 = np.array([35.64, 65.38, 38.88, 49.55, 69.67, 47.81, 63.88, 62.20, 65.38, 49.64])/100.0\n",
        "mean_L1 = np.sum(accuracy_L1)/accuracy_L1.shape[0]\n",
        "standard_dev_L1 = np.sqrt(np.sum((accuracy_L1-mean_L1)**2)/(accuracy_L1.shape[0]-1.0))\n",
        "standard_error_L1 = standard_dev_L1/np.sqrt(accuracy_L1.shape[0])\n",
        "variance_L1 = standard_dev_L1**2\n",
        "\n",
        "accuracy_L2 = np.array([64.52, 67.95, 58.26, 64.46, 56.47, 63.63, 57.30, 54.49, 60.33, 62.54])/100.0\n",
        "mean_L2 = np.sum(accuracy_L2)/accuracy_L2.shape[0]\n",
        "standard_dev_L2 = np.sqrt(np.sum((accuracy_L2-mean_L2)**2)/(accuracy_L2.shape[0]-1.0))\n",
        "standard_error_L2 = standard_dev_L2/np.sqrt(accuracy_L2.shape[0])\n",
        "variance_L2 = standard_dev_L2**2\n",
        "\n",
        "accuracy_optimised = np.array([69.52, 67.95, 58.26, 74.46, 62.47, 63.63, 67.30, 68.49, 70.84, 71.58])/100.0\n",
        "mean_optimised = np.sum(accuracy_optimised)/accuracy_optimised.shape[0]\n",
        "standard_dev_optimised = np.sqrt(np.sum((accuracy_optimised-mean_optimised)**2)/(accuracy_optimised.shape[0]-1.0))\n",
        "standard_error_optimised = standard_dev_optimised/np.sqrt(accuracy_optimised.shape[0])\n",
        "variance_optimised = standard_dev_optimised**2\n",
        "\n",
        "x = np.array([0,1,2,3])\n",
        "y_mean = np.array([mean_base, mean_L1, mean_L2, mean_optimised])\n",
        "y_standard_error = np.array([standard_error_base, standard_error_L1, standard_error_L2, standard_error_optimised])\n",
        "y_variance = np.array([variance_base, variance_L1, variance_L2,variance_optimised])\n",
        "\n",
        "plt.figure(0)\n",
        "my_xticks = ['Without Regularization','L1','L2','optimised']\n",
        "plt.plot(x, y_mean, 'bo', label='Mean Accuracy')\n",
        "plt.plot(x, y_standard_error, 'ro', label='Mean Standard Error')\n",
        "plt.plot(x, y_variance, 'go', label='Mean Variance')\n",
        "plt.xticks(x, my_xticks)\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Test accuracy')\n",
        "plt.title('MNIST-784 dataset')\n",
        "plt.legend()\n",
        "plt.savefig('MNIST_plots.jpg',dpi=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "j-EZp7CRKOyV",
        "outputId": "950c4c25-ef52-4816-ed3e-ac633a08f60a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(mean_L1-mean_base)*100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48jjHN6kKOvP",
        "outputId": "f0f367ec-6614-4ed7-83bc-4095441cff85"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.956000000000012"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(mean_base-mean_optimised)*100"
      ],
      "metadata": {
        "id": "Qr7dJFWNinN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e453bb50-6935-4d57-e8fe-ea06829aeaa0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-19.60300000000001"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "accuracy_base = np.array([51.10, 42.24, 44.21, 44.21, 53.20, 46.30, 47.33, 46.23, 48.23, 31.46])/100.0\n",
        "mean_base = np.sum(accuracy_base)/accuracy_base.shape[0]\n",
        "standard_dev_base = np.sqrt(np.sum((accuracy_base-mean_base)**2)/(accuracy_base.shape[0]-1.0))\n",
        "standard_error_base = standard_dev_base/np.sqrt(accuracy_base.shape[0])\n",
        "variance_base = standard_dev_base**2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "accuracy_L1 = np.array([65.45, 32.85, 45.81, 37.45, 64.67, 67.13, 62.28, 62.90, 52.86, 65.64])/100.0\n",
        "mean_L1 = np.sum(accuracy_L1)/accuracy_L1.shape[0]\n",
        "standard_dev_L1 = np.sqrt(np.sum((accuracy_L1-mean_L1)**2)/(accuracy_L1.shape[0]-1.0))\n",
        "standard_error_L1 = standard_dev_L1/np.sqrt(accuracy_L1.shape[0])\n",
        "variance_L1 = standard_dev_L1**2\n",
        "\n",
        "accuracy_L2 = np.array([63.23, 64.54, 64.64, 58.36, 62.37, 56.33, 54.04, 53.93, 61.13, 62.23])/100.0\n",
        "mean_L2 = np.sum(accuracy_L2)/accuracy_L2.shape[0]\n",
        "standard_dev_L2 = np.sqrt(np.sum((accuracy_L2-mean_L2)**2)/(accuracy_L2.shape[0]-1.0))\n",
        "standard_error_L2 = standard_dev_L2/np.sqrt(accuracy_L2.shape[0])\n",
        "variance_L2 = standard_dev_L2**2\n",
        "\n",
        "accuracy_optimised = np.array([68.24, 68.59, 74.63, 53.46, 60.74, 63.39, 63.03, 64.49, 72.44, 68.84])/100.0\n",
        "mean_optimised = np.sum(accuracy_optimised)/accuracy_optimised.shape[0]\n",
        "standard_dev_optimised = np.sqrt(np.sum((accuracy_optimised-mean_optimised)**2)/(accuracy_optimised.shape[0]-1.0))\n",
        "standard_error_optimised = standard_dev_optimised/np.sqrt(accuracy_optimised.shape[0])\n",
        "variance_optimised = standard_dev_optimised**2\n",
        "\n",
        "x = np.array([0,1,2,3])\n",
        "y_mean = np.array([mean_base, mean_L1, mean_L2, mean_optimised])\n",
        "y_standard_error = np.array([standard_error_base, standard_error_L1, standard_error_L2, standard_error_optimised])\n",
        "y_variance = np.array([variance_base, variance_L1, variance_L2,variance_optimised])\n",
        "\n",
        "plt.figure(0)\n",
        "my_xticks = ['Without Regularization','L1','L2','optimised']\n",
        "plt.plot(x, y_mean, 'bo', label='Mean Accuracy')\n",
        "plt.plot(x, y_standard_error, 'ro', label='Mean Standard Error')\n",
        "plt.plot(x, y_variance, 'go', label='Mean Variance')\n",
        "plt.xticks(x, my_xticks)\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Test accuracy')\n",
        "plt.title('MNIST-Fashion dataset')\n",
        "plt.legend()\n",
        "plt.savefig('MNIST_plots.jpg',dpi=200)"
      ],
      "metadata": {
        "id": "NfDD77Vgt1vO",
        "outputId": "51b0b968-14d0-4a44-9fa4-c79c47d6deab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(mean_base-mean_L1)*100"
      ],
      "metadata": {
        "id": "9p3QMu5O1Y_D",
        "outputId": "60e38cbd-817a-4a66-e700-05710c8e35dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-10.253"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(mean_base-mean_L2)*100"
      ],
      "metadata": {
        "id": "NilgikJA6naQ",
        "outputId": "d95f5c5e-f839-49cb-e417-cce5d76521a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-14.629000000000003"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(mean_base-mean_optimised)*100"
      ],
      "metadata": {
        "id": "54ei2QU_7pal",
        "outputId": "86eac844-6501-41e7-f079-99be095f235d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-20.333999999999996"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_CLChrL58J5f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}